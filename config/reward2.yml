environment:  
  p_num: 10
  v_num: 30
  var: 0.01
  service_rate: 100
  arrival_rate: 0.182 # 100% system load: p_num / distribution expectation / service rate 
  training_steps: 500
  eval_steps: 150000
  seed: 0
  reward_function: 2
  cap_target_util: true
  sequence: "uniform"
  beta: 0.5
agents: 
  dqn:
      n_episodes: 2000
      batch_size: 100
      memory_size: 10000
      gamma:  0.99
      eps_start: 1
      eps_end: 0.1
      eps_decay: 400
      target_update: 40                 # typically 1/5 of n_episodes
      hidden_size_1: 64
      hidden_size_2: 64
      learning_rate:  1e-4
      show_training_progress: true
  ppo: 
      n_episodes: 2000
      hidden_size_1: 64
      hidden_size_2: 64
      lr: 3e-5
      lr_lambda: 0.9999
      gamma: 0.99
      lamda: 0.98
      ent_coef: 0.01
      vf_coef: 0.5             
      k_epochs: 4       
      kl_max: 0.02              # early stop to prevent big kl         
      eps_clip: 0.1                              # Clip range
      max_grad_norm: 0.5
      batch_size: 100                   # Update every batch_size samples
      minibatch_size: 25               # Divide batch into smaller chunks
      det_eval: false                    # Determinisitc action for evaludation
      network_arch: separate
      reward_scaling: false
      show_training_progress: true